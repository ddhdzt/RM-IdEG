{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:40:36.573618Z",
     "start_time": "2019-07-01T16:40:34.810452Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import os, pickle\n",
    "\n",
    "# specify the GPU device\n",
    "# os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:40:36.583694Z",
     "start_time": "2019-07-01T16:40:36.580048Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Config\n",
    "'''\n",
    "# batch size per iteration\n",
    "BATCHSIZE = 400\n",
    "# mini-batch size for few-shot learning\n",
    "# MINIBATCHSIZE = 10\n",
    "# learning rate\n",
    "LR = 1e-3\n",
    "# coefficient to balance `cold-start' and `warm-up'\n",
    "ALPHA = 0.1\n",
    "# length of embedding vectors\n",
    "EMB_SIZE = 128\n",
    "# model\n",
    "MODEL = 'DisNet'\n",
    "# log file\n",
    "LOG = \"logs/{}.csv\".format(MODEL)\n",
    "# path to save the model\n",
    "saver_path =\"saver/model-\"+LOG.split(\"/\")[-1][:-4]\n",
    "# Size of latent vectors\n",
    "LATENT_SIZE = 64\n",
    "# ISO type\n",
    "ISO = 'nn'\n",
    "#Title length\n",
    "title_len = 100\n",
    "num_rel_items = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上方代码配置超参数和存放的路径，这里有个Alpha是损失函数里的权衡参数，EMB_SIZE是Embedding的维度，这里选择DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:40:36.593151Z",
     "start_time": "2019-07-01T16:40:36.585458Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        t = pickle.load(f)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上方是读入数据的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:40:41.604033Z",
     "start_time": "2019-07-01T16:40:36.595962Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# some pre-processing\n",
    "num_words_dict = {\n",
    "    'User-ID': 92108, \n",
    "    'ISBN': 270171, \n",
    "    'Location': 22449, \n",
    "    'Age': 142, \n",
    "    'Book-Author': 101582, \n",
    "    'Year-Of-Publication': 117, \n",
    "    'Publisher': 16729\n",
    "}\n",
    "# 'User-ID', 'ISBN', 'Location', 'Age', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher', 'label'\n",
    "ID_col = 'ISBN'\n",
    "item_col = ['Book-Author', 'Year-Of-Publication', 'Publisher']\n",
    "context_col = ['User-ID', 'Age', 'Location']\n",
    "real_con_cols_num = 1\n",
    "# context_col = ['Age', 'Gender', 'UserID', 'Occupation']\n",
    "\n",
    "def get_data(file):\n",
    "    data = read_pkl(file)\n",
    "    y = np.array(list(data['label']))\n",
    "    title = np.array(list(map(lambda x:  list(x), data['Book-Title'].values)))\n",
    "    title_len = title.shape[1]\n",
    "    feature = data[[ID_col]+item_col+context_col]\n",
    "    return feature, title, y\n",
    "\n",
    "rel_item_col = \"rel_item_ids\"\n",
    "def get_rel_data(file):\n",
    "    global num_rel_items\n",
    "    data = read_pkl(file)\n",
    "    y = np.array(list(data['label']))\n",
    "    rel_item = np.array(list(data[rel_item_col]), dtype=np.int32)\n",
    "    num_rel_items = rel_item.shape[1]\n",
    "    title = np.array(list(map(lambda x:  list(x), data['Book-Title'].values)))\n",
    "    title_len = title.shape[1]\n",
    "    feature = data[[ID_col]+item_col+context_col]\n",
    "    return feature, title, rel_item, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "big_train_main应该是用来训练base模型的\n",
    "\n",
    "pad_sequences可以把序列数据进行截断或者补齐，Genres为什么要这么做呢？\n",
    "\n",
    "训练数据基本就是ID+item_col+context_col，和文章对应\n",
    "\n",
    "感觉train_t和train_g也是item的属性，属于item_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_x, pre_t, pre_y = get_data(\"./disnet_data/data_pretrain.pkl\")\n",
    "pop_x_train, pop_t_train, pop_y_train = get_data(\"./disnet_data/train_pop.pkl\")\n",
    "pop_x_test, pop_t_test, pop_y_test = get_data(\"./disnet_data/test_pop.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是读入其他训练数据，a,b,c看起来是预热用来更新Embedding的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:40:42.825843Z",
     "start_time": "2019-07-01T16:40:42.804600Z"
    },
    "code_folding": [
     82
    ]
   },
   "outputs": [],
   "source": [
    "class Meta_Model(object):\n",
    "    def __init__(self, ID_col, item_col, context_col, nb_words, model='FM',\n",
    "                 emb_size=128, alpha=0.1,\n",
    "                 warm_lr=1e-3, cold_lr=1e-4, ME_lr=1e-3, latent_size=16, is_pretrain=True, is_meta=False,\n",
    "                REG=0.5):\n",
    "        \"\"\"\n",
    "        ID_col: string, the column name of the item ID\n",
    "        item_col: list, the columns of item features\n",
    "        context_col: list, the columns of other features\n",
    "        nb_words: dict, nb of words in each of these columns\n",
    "        \"\"\"\n",
    "        tf.reset_default_graph() \n",
    "        self.dropout_keep = tf.placeholder(tf.float32)\n",
    "        all_tables = []\n",
    "        self.flag = True\n",
    "        self.is_pretrain = is_pretrain\n",
    "        self.is_meta = is_meta\n",
    "        columns = [ID_col] + item_col + context_col\n",
    "        inputs_b = None\n",
    "        warm_loss = None\n",
    "        if self.is_pretrain:\n",
    "            columns = [ID_col] + item_col + context_col[:-real_con_cols_num]\n",
    "        else:\n",
    "            columns = [ID_col] + item_col + context_col\n",
    "        def get_embeddings():\n",
    "            inputs, tables = {}, []\n",
    "            item_embs, other_embs = [], []\n",
    "            with tf.variable_scope(\"embeddings\", reuse=tf.AUTO_REUSE):\n",
    "                # All the embeddings are reusable\n",
    "                for col in columns:\n",
    "                    inputs[col] = tf.placeholder(tf.int32, [None])\n",
    "                    table = tf.get_variable(\n",
    "                        \"table_{}\".format(col), [nb_words[col], emb_size],\n",
    "                        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "                    if self.flag:\n",
    "                        all_tables.append(table)\n",
    "                    emb = tf.nn.embedding_lookup(table, inputs[col])\n",
    "                    if col==ID_col:\n",
    "                        ID_emb = emb\n",
    "                        ID_table = table\n",
    "                    elif col in item_col:\n",
    "                        item_embs.append(emb)\n",
    "                    else:\n",
    "                        other_embs.append(emb)\n",
    "                inputs[\"title\"] = tf.placeholder(tf.float32, [None, 100])\n",
    "                inputs[\"rel_item_ids\"] = tf.placeholder(tf.int32, [None, num_rel_items])\n",
    "            self.flag = False\n",
    "            \n",
    "            return inputs, ID_emb, item_embs, other_embs, ID_table\n",
    "        \n",
    "        def generate_rel_meta_emb(item_embs, rel_id_embs, title):\n",
    "            ''' 生成relational meta embedding\n",
    "            '''\n",
    "            with tf.variable_scope(\"meta_embeddings\", reuse=tf.AUTO_REUSE):\n",
    "                \"\"\"\n",
    "                This is the simplest architecture of the embedding generator,\n",
    "                with only a dense layer.\n",
    "                You can customize it if you want to get a stronger performance, \n",
    "                for example, you can add an l2 regularization term or alter \n",
    "                the pooling layer. \n",
    "                \"\"\"\n",
    "                embs = tf.stop_gradient(tf.stack(item_embs, 1))\n",
    "                item_h = tf.layers.flatten(embs)\n",
    "                \n",
    "                # attention part, item_h: [None, M], rel_id_embs:[None, 10, emb_size]\n",
    "                emb_size = 128\n",
    "                hidden_size = 64\n",
    "                attention_w = tf.get_variable(\"attention_w\", [emb_size, hidden_size])\n",
    "                attention_h = tf.get_variable(\"attention_h\", [hidden_size])\n",
    "                attention_b = tf.get_variable(\"attention_b\", [hidden_size])\n",
    "                attention_mul = tf.nn.relu(tf.matmul(rel_id_embs, attention_w) + attention_b)\n",
    "                attention_relu = tf.reduce_sum(tf.multiply(attention_h, attention_mul), 2, keepdims=True)\n",
    "                attention_score = tf.nn.softmax(attention_relu)\n",
    "                # rel_id_embs: [None, 10]\n",
    "                rel_id_embs = tf.reduce_sum(tf.multiply(attention_score, rel_id_embs), 2) / 10\n",
    "                item_h = tf.concat([item_h, title, rel_id_embs], axis=-1)\n",
    "                \n",
    "                emb_pred_Dense1 = Dense(\n",
    "                    emb_size, activation=tf.nn.relu, use_bias=True,\n",
    "                    name='emb_predictor1')\n",
    "                \n",
    "                emb_pred_Dense2 = Dense(\n",
    "                    emb_size, activation=tf.nn.tanh, use_bias=False,\n",
    "                    name='emb_predictor2')\n",
    "                \n",
    "                emb_pred = emb_pred_Dense2(emb_pred_Dense1(item_h)) / 5.\n",
    "                ME_vars = [attention_w, attention_h, attention_b]\\\n",
    "                        + emb_pred_Dense1.trainable_variables + emb_pred_Dense2.trainable_variables\n",
    "#                 ME_vars.extend([attention_w, attention_h, attention_b])\n",
    "                return emb_pred, ME_vars\n",
    "        \n",
    "        def generate_meta_emb(item_embs, title):\n",
    "            with tf.variable_scope(\"meta_embeddings\", reuse=tf.AUTO_REUSE):\n",
    "                \"\"\"\n",
    "                This is the simplest architecture of the embedding generator,\n",
    "                with only a dense layer.\n",
    "                You can customize it if you want to get a stronger performance, \n",
    "                for example, you can add an l2 regularization term or alter \n",
    "                the pooling layer. \n",
    "                \"\"\"\n",
    "                embs = tf.stop_gradient(tf.stack(item_embs, 1))\n",
    "                item_h = tf.layers.flatten(embs)\n",
    "                item_h = tf.concat([item_h, title], axis=1)\n",
    "                emb_pred_Dense = Dense(\n",
    "                    emb_size, activation=tf.nn.tanh, use_bias=False,\n",
    "                    name='emb_predictor') \n",
    "                emb_pred = emb_pred_Dense(item_h) / 5.\n",
    "                ME_vars = emb_pred_Dense.trainable_variables\n",
    "                return emb_pred, ME_vars\n",
    "\n",
    "        def get_yhat_DisNet(ID_emb, item_embs, other_embs, title, **kwargs):\n",
    "            item_input = tf.concat([ID_emb] + item_embs + [title], -1)\n",
    "            if self.is_pretrain:\n",
    "                user_input = tf.concat(other_embs, -1)\n",
    "                # context has been removed in advance\n",
    "            else:\n",
    "                user_input = tf.concat(other_embs[:-real_con_cols_num], -1)\n",
    "                # if not pretrain, remove the context\n",
    "            \n",
    "            with tf.variable_scope(\"DisNet_user_net\", reuse=tf.AUTO_REUSE):\n",
    "                user_interest = tf.nn.relu(tf.layers.dense(user_input, latent_size, name='user_net_1'))\n",
    "                user_interest = tf.layers.dense(user_interest, latent_size, name='user_net_2')\n",
    "            \n",
    "            with tf.variable_scope(\"DisNet_item_net\", reuse=tf.AUTO_REUSE):\n",
    "                item_interest = tf.nn.relu(tf.layers.dense(item_input, latent_size, name='item_net_1'))\n",
    "                item_interest = tf.layers.dense(item_interest, latent_size, name='item_net_2')\n",
    "            \n",
    "            if self.is_pretrain == False:\n",
    "                with tf.variable_scope(\"context_net\", reuse=tf.AUTO_REUSE):\n",
    "                    context_input = tf.concat(other_embs[-real_con_cols_num:], -1)\n",
    "                    print('context_input: ', context_input)\n",
    "                    # load time_stamp\n",
    "                    if ISO == 'add':\n",
    "                        interest_shifting_vector = tf.nn.relu(tf.layers.dense(context_input, latent_size, name='con_net_1'))\n",
    "                        interest_shifting_vector = tf.nn.relu(tf.layers.dense(context_input, latent_size, name='con_net_2'))\n",
    "                        interest_shifting_vector = tf.layers.dense(interest_shifting_vector, latent_size, name='con_net_3')\n",
    "                        user_interest = user_interest + interest_shifting_vector\n",
    "                    elif ISO == 'nn':\n",
    "                        interest_shifting_vector = tf.nn.relu(tf.layers.dense(context_input, latent_size, name='con_net_1'))\n",
    "                        interest_shifting_vector = tf.layers.dense(interest_shifting_vector, latent_size, name='con_net_2')\n",
    "                        concated_vectors = tf.concat([user_interest, interest_shifting_vector], 1)\n",
    "                        user_interest = tf.nn.relu(concated_vectors)\n",
    "                        user_interest = tf.layers.dense(user_interest, latent_size, name='iso_net_2')\n",
    "                    elif ISO == 'cot':\n",
    "                        interest_shifting_vector = tf.nn.relu(tf.layers.dense(context_input, latent_size * latent_size, name='con_net_1'))\n",
    "                        interest_shifting_vector = tf.layers.dense(interest_shifting_vector, latent_size * latent_size, name='con_net_2')\n",
    "                        interest_shifting_vector = tf.reshape(interest_shifting_vector, [-1, latent_size, latent_size])\n",
    "                        user_interest = tf.matmul(interest_shifting_vector, tf.expand_dims(user_interest, 2))\n",
    "                        user_interest = tf.reshape(user_interest, [-1, latent_size])\n",
    "            \n",
    "            with tf.variable_scope(\"merge_net\", reuse=tf.AUTO_REUSE):\n",
    "                h_deep = tf.nn.relu(tf.concat([item_interest, user_interest], axis=1))\n",
    "                h_deep = tf.nn.relu(tf.layers.dense(h_deep, latent_size, name='merge_net_1'))\n",
    "                y_deep = tf.layers.dense(h_deep, 1, name='merge_net_2')\n",
    "            \n",
    "            with tf.variable_scope(\"FM_part\"):\n",
    "                embeddings = [ID_emb] + item_embs + other_embs\n",
    "                sum_of_emb = tf.add_n(embeddings)\n",
    "                diff_of_emb = [sum_of_emb - x for x in embeddings]\n",
    "                # n*t\n",
    "                dot_of_emb = [tf.reduce_sum(embeddings[i]*diff_of_emb[i], \n",
    "                                            axis=1, keepdims=True) \n",
    "                              for i in range(len(columns))]\n",
    "                h_fm = tf.concat(dot_of_emb, 1)\n",
    "                y_fm = tf.reduce_sum(h_fm, axis=1, keepdims=True)\n",
    "            \n",
    "            with tf.variable_scope(\"output_part\"):\n",
    "                y = tf.nn.sigmoid(y_deep)\n",
    "            \n",
    "            return y\n",
    "        \n",
    "        def get_yhat_deepFM(ID_emb, item_embs, other_embs, title, **kwargs):\n",
    "            with tf.variable_scope(\"context_models\"):\n",
    "                embeddings = [ID_emb] + item_embs + other_embs\n",
    "                sum_of_emb = tf.add_n(embeddings)\n",
    "                diff_of_emb = [sum_of_emb - x for x in embeddings]\n",
    "                dot_of_emb = [tf.reduce_sum(embeddings[i]*diff_of_emb[i], \n",
    "                                            axis=1, keepdims=True) \n",
    "                              for i in range(len(columns))]\n",
    "                y_fm = tf.reduce_sum(tf.concat(dot_of_emb, 1), axis=1, keepdims=True)\n",
    "                \n",
    "                h2 = tf.concat(embeddings + [title], 1)\n",
    "                for i in range(3):\n",
    "                    h2 = tf.nn.relu(tf.layers.dense(h2, emb_size, name='deep-{}'.format(i)))\n",
    "                y_deep = tf.layers.dense(h2, 1, name='deep-out')\n",
    "                \n",
    "                y = tf.nn.sigmoid(y_deep + y_fm)\n",
    "                return y\n",
    "        # DeepFM实现\n",
    "        \n",
    "\n",
    "        '''\n",
    "        *CHOOSE THE BASE MODEL HERE*\n",
    "        '''\n",
    "        get_yhat = {\n",
    "            \"deepFM\": get_yhat_deepFM,\n",
    "            'DisNet': get_yhat_DisNet,\n",
    "        }[model]\n",
    "        \n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        reg_losses = tf.reduce_sum(reg_losses) * REG\n",
    "    \n",
    "        if self.is_meta == False:\n",
    "            if self.is_pretrain:\n",
    "                with tf.variable_scope(\"base_model\", reuse=tf.AUTO_REUSE):\n",
    "                    # build the base model\n",
    "                    inputs, ID_emb, item_embs, other_embs, ID_table = get_embeddings()\n",
    "                    label = tf.placeholder(tf.float32, [None, 1])\n",
    "                    yhat = get_yhat(ID_emb, item_embs, other_embs, inputs['title'])\n",
    "                    warm_loss = tf.losses.log_loss(label, yhat) + reg_losses\n",
    "                    # Meta-Embedding: build the embedding generator\n",
    "                    # build the optimizer and update op for the original model\n",
    "                    warm_optimizer = tf.train.AdamOptimizer(warm_lr)\n",
    "                    warm_update_op = warm_optimizer.minimize(warm_loss)\n",
    "                    warm_update_emb_op = warm_optimizer.minimize(warm_loss, var_list=[ID_table])\n",
    "            else:\n",
    "                with tf.variable_scope(\"base_model\", reuse=tf.AUTO_REUSE):\n",
    "                    # build the base model\n",
    "                    inputs, ID_emb, item_embs, other_embs, ID_table = get_embeddings()\n",
    "                    label = tf.placeholder(tf.float32, [None, 1])\n",
    "                    # outputs and losses of the base model\n",
    "                    yhat = get_yhat(ID_emb, item_embs, other_embs, inputs['title'])\n",
    "                    warm_loss = tf.losses.log_loss(label, yhat) + reg_losses\n",
    "                    # Meta-Embedding: build the embedding generator\n",
    "                    # build the optimizer and update op for the original model\n",
    "                    warm_optimizer = tf.train.AdamOptimizer(warm_lr)\n",
    "                    var_list = all_tables\n",
    "                    for var in tf.global_variables():\n",
    "                        if (str(var).find('Adam') < 0 and str(var).find('beta') < 0 \\\n",
    "                            and str(var).find('context_net') < 0 and str(var).find('merge_net') < 0) == False:\n",
    "                            print('Collecting shifting vars: ', var)\n",
    "                            var_list.append(var)\n",
    "                    warm_update_op = warm_optimizer.minimize(warm_loss)\n",
    "                    warm_update_emb_op = warm_optimizer.minimize(warm_loss,  var_list=[ID_table])\n",
    "        else:\n",
    "            with tf.variable_scope(\"base_model\", reuse=tf.AUTO_REUSE):\n",
    "                inputs, ID_emb, item_embs, other_embs, ID_table = get_embeddings()\n",
    "                label = tf.placeholder(tf.float32, [None, 1])\n",
    "                # outputs and losses of the base model\n",
    "                yhat = get_yhat(ID_emb, item_embs, other_embs, inputs['title'])\n",
    "                warm_loss = tf.losses.log_loss(label, yhat) + reg_losses\n",
    "                warm_optimizer = tf.train.AdamOptimizer(warm_lr)\n",
    "                warm_update_emb_op = warm_optimizer.minimize(warm_loss, var_list=[ID_table])\n",
    "\n",
    "                rel_id_embs = tf.nn.embedding_lookup(ID_table, inputs[\"rel_item_ids\"])\n",
    "                meta_ID_emb, ME_vars = generate_rel_meta_emb(item_embs, rel_id_embs, inputs['title'])\n",
    "\n",
    "                # Meta-Embedding: step 1, cold-start, \n",
    "                #     use the generated meta-embedding to make predictions\n",
    "                #     and calculate the cold-start loss_a\n",
    "                cold_yhat_a = get_yhat(meta_ID_emb, item_embs, other_embs, inputs['title'])\n",
    "                cold_loss_a = tf.losses.log_loss(label, cold_yhat_a)\n",
    "\n",
    "                # Meta-Embedding: step 2, apply gradient descent once\n",
    "                #     get the adapted embedding\n",
    "                cold_emb_grads = tf.gradients(cold_loss_a, meta_ID_emb)[0]\n",
    "                meta_ID_emb_new = meta_ID_emb - cold_lr * cold_emb_grads\n",
    "\n",
    "                # Meta-Embedding: step 3, \n",
    "                #     use the adapted embedding to make prediction on another mini-batch \n",
    "                #     and calculate the warm-up loss_b\n",
    "                inputs_b, _, item_embs_b, other_embs_b, _ = get_embeddings()\n",
    "                label_b = tf.placeholder(tf.float32, [None, 1])\n",
    "                cold_yhat_b = get_yhat(meta_ID_emb_new, item_embs_b, other_embs_b, inputs_b['title'])\n",
    "                cold_loss_b = tf.losses.log_loss(label_b, cold_yhat_b)\n",
    "\n",
    "                # build the optimizer and update op for meta-embedding\n",
    "                # Meta-Embedding: step 4, calculate the final meta-loss\n",
    "                ME_loss = cold_loss_b * (1-alpha) + cold_loss_a * alpha + reg_losses\n",
    "                # ME_loss = cold_loss_b + reg_losses\n",
    "                ME_optimizer = tf.train.AdamOptimizer(ME_lr)\n",
    "                ME_update_op = ME_optimizer.minimize(cold_loss_b, var_list=ME_vars)\n",
    "         \n",
    "        ID_table_new = tf.placeholder(tf.float32, ID_table.shape)\n",
    "        ME_assign_op = tf.assign(ID_table, ID_table_new)\n",
    "        \n",
    "        def predict_warm(sess, X, Title):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         self.dropout_keep: 1.0,\n",
    "                         **feed_dict}\n",
    "            return sess.run(yhat, feed_dict)\n",
    "        \n",
    "        def predict_ME(sess, X, Title):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         self.dropout_keep: 1.0,\n",
    "                         **feed_dict}\n",
    "            return sess.run(cold_yhat_a, feed_dict)\n",
    "        \n",
    "        # log: Rel_Predict_ME\n",
    "        def Predict_Rel_ME(sess, X, Title, rel_item_ids):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"rel_item_ids\"]: rel_item_ids,\n",
    "                         self.dropout_keep: 1.0,\n",
    "                         **feed_dict}\n",
    "            return sess.run(cold_yhat_a, feed_dict)\n",
    "        \n",
    "        def get_rel_meta_embedding(sess, X, Title, rel_item_ids):\n",
    "            feed_dict = {inputs[col]: np.reshape(X[col], (-1)) for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: np.reshape(Title, (-1, title_len)),\n",
    "                         inputs[\"rel_item_ids\"]: rel_item_ids,\n",
    "                         self.dropout_keep: 1.0,\n",
    "                         **feed_dict}\n",
    "            return sess.run(meta_ID_emb, feed_dict)\n",
    "        \n",
    "        def get_meta_embedding(sess, X, Title):\n",
    "            feed_dict = {inputs[col]: np.reshape(X[col], (-1)) for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: np.reshape(Title, (-1, title_len)),\n",
    "                         self.dropout_keep: 1.0,\n",
    "                         **feed_dict}\n",
    "            return sess.run(meta_ID_emb, feed_dict)\n",
    "        \n",
    "        def assign_meta_embedding(sess, IDs, emb):\n",
    "            # take the embedding matrix\n",
    "            table = sess.run(ID_table)\n",
    "            # replace the ID^th row by the new embedding\n",
    "            for i in range(len(IDs)):\n",
    "                table[IDs[i], :] = emb[i]\n",
    "            return sess.run(ME_assign_op, feed_dict={ID_table_new: table})\n",
    "        \n",
    "#         def assign_meta_embedding(sess, ID, emb):\n",
    "#             # take the embedding matrix\n",
    "#             table = sess.run(ID_table)\n",
    "#             # replace the ID^th row by the new embedding\n",
    "#             table[ID, :] = emb\n",
    "#             return sess.run(ME_assign_op, feed_dict={ID_table_new: table})\n",
    "        \n",
    "        def train_pretrain(sess, X, Title, y, embedding_only=False):\n",
    "            # original training on batch\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         self.dropout_keep: 0.8,\n",
    "                         **feed_dict}\n",
    "            feed_dict[label] = y.reshape((-1,1))\n",
    "            return sess.run([\n",
    "                warm_loss, warm_update_emb_op if embedding_only else warm_update_op \n",
    "            ], feed_dict=feed_dict)\n",
    "        \n",
    "        def train_warm(sess, X, Title, y, embedding_only=False):\n",
    "            # original training on batch\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         self.dropout_keep: 0.8,\n",
    "                         **feed_dict}\n",
    "            feed_dict[label] = y.reshape((-1,1))\n",
    "            \n",
    "            return sess.run([\n",
    "                warm_loss, warm_update_emb_op if embedding_only else warm_update_op \n",
    "            ], feed_dict=feed_dict)\n",
    "        \n",
    "        def train_ME(sess, X, Title, y, \n",
    "                     X_b, Title_b, y_b):\n",
    "            # train the embedding generator\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         self.dropout_keep: 0.8,\n",
    "                         **feed_dict}\n",
    "            feed_dict[label] = y.reshape((-1,1))\n",
    "            feed_dict_b = {inputs_b[col]: X_b[col] for col in columns}\n",
    "            feed_dict_b = {inputs_b[\"title\"]: Title_b,\n",
    "                           **feed_dict_b}\n",
    "            feed_dict_b[label_b] = y_b.reshape((-1,1))\n",
    "            return sess.run([\n",
    "                cold_loss_a, cold_loss_b, ME_update_op\n",
    "            ], feed_dict={**feed_dict, **feed_dict_b})\n",
    "        \n",
    "        # log: train_Rel_ME\n",
    "        def train_Rel_ME(sess, X, Title, rel_item_id, y, \n",
    "                     X_b, Title_b, rel_item_id_b, y_b):\n",
    "            # train the embedding generator\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"rel_item_ids\"]: rel_item_id, # log: rel_item_id\n",
    "                         self.dropout_keep: 0.8,\n",
    "                         **feed_dict}\n",
    "            feed_dict[label] = y.reshape((-1,1))\n",
    "            feed_dict_b = {inputs_b[col]: X_b[col] for col in columns}\n",
    "            feed_dict_b = {inputs_b[\"title\"]: Title_b,\n",
    "                           inputs[\"rel_item_ids\"]: rel_item_id_b, # log: rel_item_id\n",
    "                           **feed_dict_b}\n",
    "            feed_dict_b[label_b] = y_b.reshape((-1,1))\n",
    "            return sess.run([\n",
    "                cold_loss_a, cold_loss_b, ME_update_op\n",
    "            ], feed_dict={**feed_dict, **feed_dict_b})\n",
    "        \n",
    "        self.predict_warm = predict_warm\n",
    "        self.predict_ME = predict_ME\n",
    "        self.predict_Rel_ME = Predict_Rel_ME\n",
    "        self.train_warm = train_warm\n",
    "        self.train_ME = train_ME\n",
    "        self.train_Rel_ME = train_Rel_ME\n",
    "        self.get_meta_embedding = get_meta_embedding\n",
    "        self.get_rel_meta_embedding = get_rel_meta_embedding\n",
    "        self.assign_meta_embedding = assign_meta_embedding\n",
    "\n",
    "    def construct_conv_layers(self, depth, embeddings, name):\n",
    "        nc = self.nc\n",
    "        layer_num = len(nc)\n",
    "        iszs = [1] + nc[:-1]\n",
    "        oszs = nc\n",
    "        P = []\n",
    "        for i in range(layer_num - 1):\n",
    "            P.append(self._conv_weight(depth[i], iszs[i], oszs[i], name+'conv_layer_%d'%i))  # first 5 layers\n",
    "        P.append(self._conv_weight(1, iszs[layer_num - 1], oszs[layer_num - 1], name+'conv_layer_output'))\n",
    "\n",
    "        # Interaction Cube\n",
    "        # self.positive_embeddings = tf.concat(self.individual_embeddings, axis=1)\n",
    "\n",
    "        # self.split = tf.split(axis=1, num_or_size_splits=self.num_field, value=self.positive_embeddings)  # split as field\n",
    "\n",
    "        split = embeddings\n",
    "\n",
    "        num_field = len(split)\n",
    "        print('Num fields: ', num_field)\n",
    "        # build interaction cube\n",
    "        for i in range(0, num_field):\n",
    "            for j in range(i + 1, num_field):\n",
    "                content = 'split[' + str(i) + ']'\n",
    "                x = eval(content)\n",
    "                content1 = 'split[' + str(j) + ']'\n",
    "                y = eval(content1)\n",
    "                relation = tf.matmul(tf.transpose(x, perm=[0, 2, 1]), y)\n",
    "                net_input = tf.expand_dims(relation, 1)\n",
    "                if i == 0 and j == 1:\n",
    "                    cube = net_input\n",
    "                else:\n",
    "                    cube = tf.concat([cube, net_input], 1)\n",
    "        positive_cube = tf.expand_dims(cube, -1)\n",
    "\n",
    "        print(positive_cube.shape)\n",
    "\n",
    "        # 3D Convolution Layers\n",
    "        layer = []\n",
    "        positive_input = positive_cube\n",
    "        i = 0\n",
    "        for p in P:\n",
    "            # convolution\n",
    "            layer.append(self._conv_layer(depth[i], positive_input, p))\n",
    "            i = i + 1\n",
    "            positive_input = layer[-1]\n",
    "            print(positive_input.shape)\n",
    "        return tf.nn.dropout(layer[-1], self.dropout_keep)\n",
    "    # functions for CFM\n",
    "\n",
    "    def weight_variable(self, shape, name):\n",
    "        return tf.get_variable(name, shape=shape, initializer=tf.initializers.constant(0.))\n",
    "\n",
    "    def bias_variable(self, shape, name):\n",
    "        return tf.get_variable(name, shape=shape, initializer=tf.initializers.constant(0.))\n",
    "\n",
    "    def _regular(self, params):\n",
    "        res = 0\n",
    "        for param in params:\n",
    "            res += tf.reduce_sum(tf.square(param[0])) + tf.reduce_sum(tf.square(param[1]))\n",
    "        return res\n",
    "\n",
    "    def _conv_weight(self, deep, isz, osz, name):\n",
    "        return (self.weight_variable([deep, 2, 2, isz, osz], name+'_weight'), self.bias_variable([osz], name+'_bias'))\n",
    "\n",
    "    def _conv_layer(self, depth, input, P):\n",
    "        '''\n",
    "        Convolution layer of 3D CNN\n",
    "        :param input:\n",
    "        :param P: weights and bias\n",
    "        :return: convolution result\n",
    "        '''\n",
    "        conv = tf.nn.conv3d(input, P[0], strides=[1, depth, 2, 2, 1], padding='VALID')\n",
    "        return tf.nn.relu(conv + P[1])  # bias_add and activate\n",
    "\n",
    "def predict_on_batch(sess, predict_func, test_x, test_t, batchsize=800):\n",
    "    n_samples_test = test_x.shape[0]\n",
    "    n_batch_test = n_samples_test//batchsize\n",
    "    test_pred = np.zeros(n_samples_test)\n",
    "    for i_batch in range(n_batch_test):\n",
    "        batch_x = test_x.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t = test_t[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        _pred = predict_func(sess, batch_x, batch_t)\n",
    "        test_pred[i_batch*batchsize:(i_batch+1)*batchsize] = _pred.reshape(-1)\n",
    "    if n_batch_test*batchsize<n_samples_test:\n",
    "        batch_x = test_x.iloc[n_batch_test*batchsize:]\n",
    "        batch_t = test_t[n_batch_test*batchsize:]\n",
    "        _pred = predict_func(sess, batch_x, batch_t)\n",
    "        test_pred[n_batch_test*batchsize:] = _pred.reshape(-1)\n",
    "    return test_pred\n",
    "\n",
    "# log: rel_predict\n",
    "def predict_rel_on_batch(sess, predict_func, test_x, test_t, test_r, batchsize=800):\n",
    "    n_samples_test = test_x.shape[0]\n",
    "    n_batch_test = n_samples_test//batchsize\n",
    "    test_pred = np.zeros(n_samples_test)\n",
    "    for i_batch in range(n_batch_test):\n",
    "        batch_x = test_x.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t = test_t[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_r = test_r[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        _pred = predict_func(sess, batch_x, batch_t, batch_r)\n",
    "        test_pred[i_batch*batchsize:(i_batch+1)*batchsize] = _pred.reshape(-1)\n",
    "    if n_batch_test*batchsize<n_samples_test:\n",
    "        batch_x = test_x.iloc[n_batch_test*batchsize:]\n",
    "        batch_t = test_t[n_batch_test*batchsize:]\n",
    "        batch_r = test_r[n_batch_test*batchsize:]\n",
    "        _pred = predict_func(sess, batch_x, batch_t, batch_r)\n",
    "        test_pred[n_batch_test*batchsize:] = _pred.reshape(-1)\n",
    "    return test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:40:42.825843Z",
     "start_time": "2019-07-01T16:40:42.804600Z"
    },
    "code_folding": [
     82
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-2ed11dbbec6b>:136: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Meta_Model(ID_col, item_col, context_col, num_words_dict, model=MODEL,\n",
    "                   emb_size=EMB_SIZE, alpha=ALPHA,\n",
    "                   warm_lr=LR, cold_lr=LR/10., ME_lr=LR, latent_size=LATENT_SIZE, is_pretrain=True, is_meta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1869 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples:  747782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1869/1869 [00:25<00:00, 71.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre-train]\n",
      "\ttest-test loss: 0.901675\n",
      "[pre-train]\n",
      "\ttest-test auc: 0.740235\n",
      "Model saved in path: saver/model-DisNet\n"
     ]
    }
   ],
   "source": [
    "# ************* Evaluating DisNet *****************\n",
    "\n",
    "\"\"\"\n",
    "Pre-train the base model\n",
    "\"\"\"\n",
    "batchsize = BATCHSIZE\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_samples = pre_x.shape[0]\n",
    "print('Num samples: ', n_samples)\n",
    "n_batch = n_samples//batchsize\n",
    "for _ in range(1):\n",
    "    for i_batch in tqdm(range(n_batch)):\n",
    "        batch_x = pre_x.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t = pre_t[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_y = pre_y[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        loss, _ = model.train_warm(sess, batch_x, batch_t, batch_y)\n",
    "\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  pop_x_test, pop_t_test)\n",
    "logloss_base_cold = test_loss_test = log_loss(pop_y_test, test_pred_test)\n",
    "print(\"[pre-train]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_cold = test_auc_test = roc_auc_score(pop_y_test, test_pred_test)\n",
    "print(\"[pre-train]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "save_path = saver.save(sess, saver_path)\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_input:  Tensor(\"base_model/context_net/concat/concat:0\", shape=(?, 128), dtype=float32)\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/context_net/con_net_1/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/context_net/con_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/context_net/con_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/context_net/con_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/context_net/iso_net_2/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/context_net/iso_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/merge_net/merge_net_1/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/merge_net/merge_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/merge_net/merge_net_2/kernel:0' shape=(64, 1) dtype=float32_ref>\n",
      "Collecting shifting vars:  <tf.Variable 'base_model/merge_net/merge_net_2/bias:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Meta_Model(ID_col, item_col, context_col, num_words_dict, model=MODEL,\n",
    "                   emb_size=EMB_SIZE, alpha=ALPHA,\n",
    "                   warm_lr=LR, cold_lr=LR/10., ME_lr=LR, latent_size=LATENT_SIZE, is_pretrain=False, is_meta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:41:30.257796Z",
     "start_time": "2019-07-01T16:40:48.579151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn DisNet False\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_ISBN:0' shape=(270171, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Book-Author:0' shape=(101582, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Year-Of-Publication:0' shape=(117, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Publisher:0' shape=(16729, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_User-ID:0' shape=(92108, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Age:0' shape=(142, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_1/kernel:0' shape=(256, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_1/kernel:0' shape=(612, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_1/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_2/kernel:0' shape=(64, 1) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_2/bias:0' shape=(1,) dtype=float32_ref>\n",
      "INFO:tensorflow:Restoring parameters from saver/model-DisNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/373 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples:  149221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:05<00:00, 65.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre-train]\n",
      "\ttest-test loss: 0.525629\n",
      "[pre-train]\n",
      "\ttest-test auc: 0.785843\n",
      "Model saved in path: saver/model-DisNet_Pop\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Popular Data\n",
    "\"\"\"\n",
    "print(ISO, MODEL, model.is_pretrain)\n",
    "batchsize = BATCHSIZE\n",
    "save_path = 'saver/model-'+MODEL\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "var_list = []\n",
    "for var in tf.global_variables():\n",
    "    if MODEL != 'CFM' and str(var).find('Adam') < 0 and str(var).find('beta') < 0 \\\n",
    "        and str(var).find('table_Location') < 0 \\\n",
    "        and str(var).find('output_part') < 0\\\n",
    "        and str(var).find('context_net') < 0\\\n",
    "        and str(var).find('context_models') < 0:\n",
    "        print('Define saver: ', var)\n",
    "        var_list.append(var)\n",
    "    \n",
    "    if MODEL == 'CFM' and str(var).find('table_Location') < 0 and str(var).find('table') >= 0:\n",
    "        var_list.append(var)\n",
    "saver = tf.train.Saver(var_list=var_list)\n",
    "saver.restore(sess, save_path)\n",
    "\n",
    "n_samples = pop_x_train.shape[0]\n",
    "print('Num samples: ', n_samples)\n",
    "n_batch = n_samples//batchsize\n",
    "for _ in range(1):\n",
    "    for i_batch in tqdm(range(n_batch)):\n",
    "        batch_x = pop_x_train.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t = pop_t_train[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_y = pop_y_train[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        loss, _ = model.train_warm(sess, batch_x, batch_t, batch_y)\n",
    "\n",
    "    test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                      pop_x_test, pop_t_test)\n",
    "    logloss_base_cold = test_loss_test = log_loss(pop_y_test, test_pred_test)\n",
    "    print(\"[pre-train]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "    auc_base_cold = test_auc_test = roc_auc_score(pop_y_test, test_pred_test)\n",
    "    print(\"[pre-train]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "saver2 = tf.train.Saver()\n",
    "save_path = save_path + '_Pop'\n",
    "save_path = saver2.save(sess, save_path)\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_input:  Tensor(\"base_model/context_net/concat/concat:0\", shape=(?, 128), dtype=float32)\n",
      "context_input:  Tensor(\"base_model/context_net_1/concat/concat:0\", shape=(?, 128), dtype=float32)\n",
      "context_input:  Tensor(\"base_model/context_net_2/concat/concat:0\", shape=(?, 128), dtype=float32)\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_ISBN:0' shape=(270171, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Book-Author:0' shape=(101582, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Year-Of-Publication:0' shape=(117, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Publisher:0' shape=(16729, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_User-ID:0' shape=(92108, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Age:0' shape=(142, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Location:0' shape=(22449, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_1/kernel:0' shape=(256, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_1/kernel:0' shape=(612, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/con_net_1/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/con_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/con_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/con_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/iso_net_2/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/iso_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_1/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_2/kernel:0' shape=(64, 1) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_2/bias:0' shape=(1,) dtype=float32_ref>\n",
      "INFO:tensorflow:Restoring parameters from saver/model-DisNet_Pop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 773/773 [00:07<00:00, 101.60it/s]\n",
      "  2%|▏         | 12/773 [00:00<00:06, 111.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\t Training loss: 0.494136\n",
      "[Meta-Embedding]\n",
      "\t Training auc: 0.797340\n",
      "[Meta-Embedding]\n",
      "\t Test loss: 0.495405\n",
      "[Meta-Embedding]\n",
      "\t Test auc: 0.794670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 773/773 [00:07<00:00, 103.01it/s]\n",
      "  1%|▏         | 11/773 [00:00<00:07, 104.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\t Training loss: 0.492623\n",
      "[Meta-Embedding]\n",
      "\t Training auc: 0.798889\n",
      "[Meta-Embedding]\n",
      "\t Test loss: 0.495321\n",
      "[Meta-Embedding]\n",
      "\t Test auc: 0.794854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 773/773 [00:07<00:00, 103.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\t Training loss: 0.490863\n",
      "[Meta-Embedding]\n",
      "\t Training auc: 0.800784\n",
      "[Meta-Embedding]\n",
      "\t Test loss: 0.495347\n",
      "[Meta-Embedding]\n",
      "\t Test auc: 0.794853\n",
      "Model saved in path: saver/model-DisNet_Rel_Meta\n"
     ]
    }
   ],
   "source": [
    "# ************* Evaluating RM-IdEG*************\n",
    "'''\n",
    "log: Train the Relational Meta-Embedding generator\n",
    "'''\n",
    "new_x_test_a, new_t_test_a, new_r_test_a, new_y_test_a = get_rel_data(\"./disnet_data/test_one_shot_a.pkl\")\n",
    "new_x_test_b, new_t_test_b, new_r_test_b, new_y_test_b = get_rel_data(\"./disnet_data/test_one_shot_b.pkl\")\n",
    "\n",
    "MINIBATCHSIZE = 50\n",
    "minibatchsize = MINIBATCHSIZE\n",
    "batch_n_ID = 1\n",
    "batchsize = MINIBATCHSIZE * batch_n_ID\n",
    "n_epoch = 1\n",
    "tf.reset_default_graph()\n",
    "model = Meta_Model(ID_col, item_col, context_col, num_words_dict, model=MODEL,\n",
    "                   emb_size=EMB_SIZE, alpha=ALPHA,\n",
    "                   warm_lr=LR, cold_lr=LR, ME_lr=LR, latent_size=LATENT_SIZE, is_pretrain=False, is_meta=True,\n",
    "                  REG=0.5)\n",
    "save_path = 'saver/model-'+MODEL + '_Pop'\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "var_list = []\n",
    "for var in tf.global_variables():\n",
    "    if MODEL != 'CFM' and str(var).find('Adam') < 0 and str(var).find('beta') < 0 \\\n",
    "        and str(var).find('meta_embeddings') < 0:\n",
    "        print('Define saver: ', var)\n",
    "        var_list.append(var)\n",
    "    \n",
    "    if MODEL == 'CFM' and str(var).find('table_Location') < 0 and str(var).find('table') >= 0:\n",
    "        var_list.append(var)\n",
    "saver = tf.train.Saver(var_list=var_list)\n",
    "saver.restore(sess, save_path)\n",
    "\n",
    "best_auc = 0\n",
    "best_loss = 10\n",
    "for i_epoch in range(n_epoch):\n",
    "    # Read the few-shot training data of big ads\n",
    "    if i_epoch==0:\n",
    "        # log: get_rel_data\n",
    "        train_x_a, train_t_a, train_r_a, train_y_a = get_rel_data(\"./disnet_data/train_one_shot_a.pkl\")\n",
    "        train_x_b, train_t_b, train_r_b, train_y_b = get_rel_data(\"./disnet_data/train_one_shot_b.pkl\")\n",
    "    \n",
    "    n_samples = train_x_a.shape[0]\n",
    "    n_batch = n_samples//batchsize\n",
    "    # Start training\n",
    "    for _ in range(3):\n",
    "        # log: add rel feature\n",
    "        for i_batch in tqdm(range(n_batch)):\n",
    "            batch_x_a = train_x_a.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "            batch_t_a = train_t_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "            batch_r_a = train_r_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "            batch_y_a = train_y_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "            batch_x_b = train_x_b.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "            batch_t_b = train_t_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "            batch_r_b = train_r_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "            batch_y_b = train_y_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "            loss_a, loss_b, _ = model.train_Rel_ME(sess, \n",
    "                                               batch_x_a, batch_t_a, batch_r_a, batch_y_a, \n",
    "                                               batch_x_b, batch_t_b, batch_r_b, batch_y_b, )\n",
    "        # on epoch end\n",
    "        # log: add rel feature\n",
    "        test_pred_test = predict_rel_on_batch(sess, model.predict_Rel_ME, \n",
    "                                          train_x_b, train_t_b, train_r_b)\n",
    "        logloss_ME_cold = test_loss_test = log_loss(train_y_b, test_pred_test)\n",
    "        print(\"[Meta-Embedding]\\n\\t Training loss: {:.6f}\".format(test_loss_test))\n",
    "        auc_ME_cold = test_auc_test = roc_auc_score(train_y_b, test_pred_test)\n",
    "        print(\"[Meta-Embedding]\\n\\t Training auc: {:.6f}\".format(test_auc_test))\n",
    "        \n",
    "        test_pred_test = predict_rel_on_batch(sess, model.predict_Rel_ME, \n",
    "                                          new_x_test_b, new_t_test_b, new_r_test_b)\n",
    "        logloss_ME_cold = test_loss_test = log_loss(new_y_test_b, test_pred_test)\n",
    "        print(\"[Meta-Embedding]\\n\\t Test loss: {:.6f}\".format(test_loss_test))\n",
    "        auc_ME_cold = test_auc_test = roc_auc_score(new_y_test_b, test_pred_test)\n",
    "        print(\"[Meta-Embedding]\\n\\t Test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "saver2 = tf.train.Saver()\n",
    "save_path = 'saver/model-'+MODEL + '_Rel_Meta'\n",
    "save_path = saver2.save(sess, save_path)\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:41:44.520743Z",
     "start_time": "2019-07-01T16:41:34.939997Z"
    },
    "code_folding": [
     76,
     93
    ]
   },
   "outputs": [],
   "source": [
    "# batchsize = 50\n",
    "# '''\n",
    "# Testing\n",
    "# '''\n",
    "# tf.reset_default_graph()\n",
    "# model = Meta_Model(ID_col, item_col, context_col, num_words_dict, model=MODEL,\n",
    "#                    emb_size=EMB_SIZE, alpha=ALPHA,\n",
    "#                    warm_lr=LR, cold_lr=LR/10., ME_lr=LR, latent_size=LATENT_SIZE, is_pretrain=False, is_meta=False)\n",
    "\n",
    "# save_path = 'saver/model-'+MODEL + '_Pop'\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# var_list = []\n",
    "# for var in tf.global_variables():\n",
    "#     if MODEL != 'CFM' and str(var).find('Adam') < 0 and str(var).find('beta') < 0:\n",
    "#         print('Define saver: ', var)\n",
    "#         var_list.append(var)\n",
    "    \n",
    "#     if MODEL == 'CFM' and str(var).find('table_Location') < 0 and str(var).find('table') >= 0:\n",
    "#         var_list.append(var)\n",
    "# saver = tf.train.Saver(var_list=var_list)\n",
    "# saver.restore(sess, save_path)\n",
    "\n",
    "# minibatchsize = MINIBATCHSIZE\n",
    "# print(batchsize)\n",
    "# i = 1\n",
    "# test_n_ID = len(new_x_test_a[ID_col].drop_duplicates())\n",
    "# print(test_n_ID)\n",
    "\n",
    "# test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "#                                   new_x_test_b, new_t_test_b)\n",
    "# logloss_base_cold = test_loss_test = log_loss(new_y_test_b, test_pred_test)\n",
    "# auc_base_cold = test_auc_test = roc_auc_score(new_y_test_b, test_pred_test)\n",
    "\n",
    "# print(\"COLD-START BASELINE:\")\n",
    "# print(\"\\t Loss: {:.4f}\".format(logloss_base_cold))\n",
    "# print(\"\\t AUC: {:.4f}\".format(auc_base_cold))\n",
    "\n",
    "# for i in tqdm(range(int(np.ceil(test_n_ID//batchsize)))):\n",
    "#     batch_x = new_x_test_a[i*batchsize:(i+1)*batchsize]\n",
    "#     batch_t = new_t_test_a[i*batchsize:(i+1)*batchsize]\n",
    "#     batch_y = new_y_test_a[i*batchsize:(i+1)*batchsize]\n",
    "#     for j in range(1):\n",
    "#         model.train_warm(sess, batch_x, batch_t, batch_y, \n",
    "#                          embedding_only=True)\n",
    "# test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "#                                   new_x_test_b, new_t_test_b)\n",
    "# logloss_base_batcha = test_loss_test = log_loss(new_y_test_b, test_pred_test)\n",
    "# print(\"[baseline]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "#     test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "# auc_base_batcha = test_auc_test = roc_auc_score(new_y_test_b, test_pred_test)\n",
    "# print(\"[baseline]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "#     test_auc_test, test_auc_test/auc_base_cold-1))\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:41:44.520743Z",
     "start_time": "2019-07-01T16:41:34.939997Z"
    },
    "code_folding": [
     76,
     93
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_input:  Tensor(\"base_model/context_net/concat/concat:0\", shape=(?, 128), dtype=float32)\n",
      "context_input:  Tensor(\"base_model/context_net_1/concat/concat:0\", shape=(?, 128), dtype=float32)\n",
      "context_input:  Tensor(\"base_model/context_net_2/concat/concat:0\", shape=(?, 128), dtype=float32)\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_ISBN:0' shape=(270171, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Book-Author:0' shape=(101582, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Year-Of-Publication:0' shape=(117, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Publisher:0' shape=(16729, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_User-ID:0' shape=(92108, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Age:0' shape=(142, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/embeddings/table_Location:0' shape=(22449, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_1/kernel:0' shape=(256, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_user_net/user_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_1/kernel:0' shape=(612, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/DisNet_item_net/item_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/con_net_1/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/con_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/con_net_2/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/con_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/iso_net_2/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/context_net/iso_net_2/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_1/kernel:0' shape=(128, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_2/kernel:0' shape=(64, 1) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/merge_net/merge_net_2/bias:0' shape=(1,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/meta_embeddings/attention_w:0' shape=(128, 64) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/meta_embeddings/attention_h:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/meta_embeddings/attention_b:0' shape=(64,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/meta_embeddings/emb_predictor1/kernel:0' shape=(494, 128) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/meta_embeddings/emb_predictor1/bias:0' shape=(128,) dtype=float32_ref>\n",
      "Define saver:  <tf.Variable 'base_model/meta_embeddings/emb_predictor2/kernel:0' shape=(128, 128) dtype=float32_ref>\n",
      "INFO:tensorflow:Restoring parameters from saver/model-DisNet_Rel_Meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/488 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss:\t0.4953, improvement: 0.27%\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc:\t0.7949, improvement: 0.11%\n",
      "0.7948531520003079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488/488 [00:42<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss:\t0.4951, improvement: 0.32%\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc:\t0.7952, improvement: 0.15%\n",
      "0.7951982267592896\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Meta_Model(ID_col, item_col, context_col, num_words_dict, model=MODEL,\n",
    "                   emb_size=EMB_SIZE, alpha=ALPHA,\n",
    "                   warm_lr=LR, cold_lr=LR/10., ME_lr=LR, latent_size=LATENT_SIZE, is_pretrain=False, is_meta=True)\n",
    "\n",
    "save_path = 'saver/model-'+MODEL + '_Rel_Meta'\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "var_list = []\n",
    "for var in tf.global_variables():\n",
    "    if str(var).find('Adam') < 0 and str(var).find('beta') < 0:\n",
    "        print('Define saver: ', var)\n",
    "        var_list.append(var)\n",
    "saver = tf.train.Saver(var_list=var_list)\n",
    "saver.restore(sess, save_path)\n",
    "\n",
    "test_pred_test = predict_rel_on_batch(sess, model.predict_Rel_ME, \n",
    "                                  new_x_test_b, new_t_test_b, new_r_test_b)\n",
    "logloss_ME_batcha = test_loss_test = log_loss(new_y_test_b, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "auc_ME_batcha = test_auc_test = roc_auc_score(new_y_test_b, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_auc_test, test_auc_test/auc_base_cold-1))\n",
    "print(auc_ME_batcha)\n",
    "\n",
    "batchsize = 20\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batchsize)))):\n",
    "    batch_x = new_x_test_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = new_t_test_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_r = new_r_test_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = new_y_test_a[i*batchsize:(i+1)*batchsize]\n",
    "    IDs = batch_x[ID_col].to_numpy()\n",
    "    embeddings = model.get_rel_meta_embedding(\n",
    "        sess, batch_x, batch_t, batch_r\n",
    "    )\n",
    "    model.assign_meta_embedding(sess, IDs, embeddings)\n",
    "    for j in range(6):\n",
    "        model.train_warm(sess, batch_x, batch_t, batch_y, \n",
    "                         embedding_only=True)\n",
    "    \n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  new_x_test_b, new_t_test_b)\n",
    "logloss_ME_batcha = test_loss_test = log_loss(new_y_test_b, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "auc_ME_batcha = test_auc_test = roc_auc_score(new_y_test_b, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_auc_test, test_auc_test/auc_base_cold-1))\n",
    "print(auc_ME_batcha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
